---
title: "ROC"
author: "Yongbock(David) Kwon"
output:
  html_document:
    keep_md: true
  html_notebook: default
  pdf_document: default
editor_options:
  chunk_output_type: console
---


## Assumtion

#### I suppose... 

#### 1 is always positive, for example, "Fraud", "Disease", or some one of binary value that has less obs than the obs of the other factor levels, so that we want to predict well. 

#### 0 is always negative, for example, "not Fraud", "not Disease", or some one of binary value that has less obs than the obs of the other factor levels. 


#### Thorough CV process and tuning parameters have been skipped since this will focus on Classification metrics. 

```{r}
library(pROC)
library(mlbench)
library(ggplot2)
library(dplyr)
library(caret)
library(MLmetrics)
library(rpart)
library(rpart.plot)
library(ROCR)
```

```{r}
set.seed(1231234)

#Let's suppose we have this binary target variables and predicted values
target <- ifelse(rbinom(1000, 1, 0.05)==1, "Fraud","not Fraud")
pred <- ifelse(rbinom(1000, 1, 0.05)==1, "Fraud", "not Fraud")

table(target, pred)

#positive = Fraud = 1
#negative = not Fraud = 0

#This function should have same levels for the both target and pred variable
tptnfpfn <- function(x,y){
  tap <- tapply(x,x,length)
  f.names <- tap[1] %>% names
  
  if(tap[1] > tap[2]){
    target <- ifelse(x == f.names, 0, 1)
    pred <- ifelse(y == f.names, 0, 1)
  }
  if(tap[2] > tap[1]){
    target <- ifelse(x == f.names, 1, 0)
    pred <- ifelse(y == f.names, 1, 0)
  }
  
  #target <- x
  #pred <- y
  
  dat <- data.frame(target, pred)
  
  TP <- length(which(dat$target == 1 & dat$pred == 1))
  FP <- length(which(dat$target == 0 & dat$pred == 1))
  TN <- length(which(dat$target == 0 & dat$pred == 0))
  FN <- length(which(dat$target == 1 & dat$pred == 0))
  
  new.dat <- data.frame(TP,FP,TN,FN)
  return(new.dat)
}

tp.dat <- tptnfpfn(target, pred)
tp.dat

```



```{r}

#Precision = TP / (TP + FP) <- the denominator is total predicted positive values
precision <- function(tp.dat){
  precision <- tp.dat$TP / (tp.dat$TP + tp.dat$FP)
  return(precision)
}


#Recall = sensitivity = TP / (TP + FN) <- the denominator is total actual positive values 

recall <- function(tp.dat){
  recall <- tp.dat$TP / (tp.dat$TP + tp.dat$FN)
  return(recall)
}

#F1 Score = 2 / (Precision^-1 + Recall^-1)
f1.score <- function(tp.dat){
  f1score <- 2/(precision(tp.dat)^(-1) + recall(tp.dat)^(-1))
  return(f1score)
}

#Sensitivity = Recall

#Specificity = TN / (TN + FP) <- the denominator is total actual negative values
spec <- function(tp.dat){
  specificity <- tp.dat$TN / (tp.dat$TN + tp.dat$FP)
  return(specificity)
}

#Syntax built in R
confusionMatrix(as.factor(pred),as.factor(target), positive="Fraud")
F1_Score(target, pred, positive = "Fraud")

#Sensitivity and Specificity by my own functions
recall(tp.dat) # = Sensitivity = TPR
spec(tp.dat) #TNR

#Precision and F1.score by my own functions
precision(tp.dat)
f1.score(tp.dat)

```



```{r}

#target: 0 = not Fraud // 1 = Fraud
target <- ifelse(rbinom(1000, 1, 0.05)==1, "Fraud","not Fraud")
#predicted values: right skewed probabilities in interval [0,1]
pred <- c(runif(900,0,0.5),runif(100,0.5,0.999))



#ROC = TPR vs FPR = Recall vs 1-TNR = TP/(TP+FN) vs FP/(FP+TN)
roc.func <- function(target,pred){
  dummy <- data.frame(TPR = rep(0, length(target)), 
                      FPR = rep(0, length(target)), 
                      Spec = rep(0,length(target)),
                      Precision = rep(0, length(target)),
                      f1score = rep(0, length(target)))
  
  tap <- tapply(target,target,length)
  if(tap[1] > tap[2]){
    f.name <- levels(as.factor(target))[2]
    s.name <- levels(as.factor(target))[1]
  }
  if(tap[2] > tap[1]){
    f.name <- levels(as.factor(target))[1]
    s.name <- levels(as.factor(target))[2]
  }
  
  for(i in 1:length(target)){
    #splitting the probabilities by cutoff with same levels
    pred.cutoff <- ifelse(pred >= sort(pred)[i], f.name, s.name)
    
    tptn <- tptnfpfn(target,pred.cutoff)
    
    dummy$cutoff[i] <- sort(pred)[i]
    dummy$TPR[i] <- recall(tptn)
    dummy$FPR[i] <- tptn$FP / (tptn$FP + tptn$TN)
    dummy$Spec[i] <- spec(tptn)
    dummy$Precision[i] <- precision(tptn)
    dummy$f1score[i] <- f1.score(tptn)
  }
  
  #dummy$TPR <- ifelse(dummy$TPR == "NaN", 0, dummy$TPR)
  #dummy$FPR <- ifelse(dummy$FPR == "NaN", 0, dummy$FPR)
  return(dummy)
}

#This auc function is from below link. 
#Refer to 
#https://mbq.me/blog/augh-roc/
#a little changes is applied into the codes from above link
#This is using the test statistic from "Mann-Whitney-Wilcoxon test"
#Further link:
#https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test#Area-under-curve_(AUC)_statistic_for_ROC_curves
auc.func <- function(target, pred){
  tap <- tapply(target, target, length)
  f.name <- tap[1] %>% names
  
  if(tap[1] > tap[2]){
    target1 <- ifelse(target == f.name, TRUE, FALSE)
  }
  if(tap[2] > tap[1]){
    target1 <- ifelse(target == f.name, TRUE, FALSE)
  }
  
  n1 <- sum(!target1)
  n2 <- sum(target1)
  U <- sum(rank(pred)[!target1]) - n1 * (n1 + 1) / 2
  
  return(1 - U / (n1*n2))
}


#Built in R
confusionMatrix(as.factor(target), as.factor(ifelse(pred >= 0.5, "Fraud", "not Fraud")))
roc.curve <- roc(target, pred, levels=c("Fraud","not Fraud"))
roc.curve
plot(roc.curve)

auc(roc.curve)

#Functions by my own
roc.dat <- roc.func(target, pred)
roc.dat %>% head

roc.dat %>% ggplot(aes(x=FPR, y=TPR)) + geom_line() + geom_abline() + labs(title="ROC Curve")
roc.dat %>% ggplot(aes(x=TPR, y=Spec)) + geom_line() + labs(title = "Sensitivity vs Specificity")
roc.dat %>% ggplot(aes(x=TPR, y=Precision)) + geom_line() + labs(title = "Precision vs Recall")
auc.func(target, pred)
```



```{r}

data(PimaIndiansDiabetes)
PimaIndiansDiabetes %>% head
pima <- PimaIndiansDiabetes
pima %>% summary

#Let's say we want to focus on predicting diabetes patients

split.indx <- createDataPartition(pima$diabetes, p=0.7, list=FALSE)

train <- pima[split.indx,]
test <- pima[-split.indx,]

```

```{r}
#modeling functions and cv control 
model <- function(method, training, control,grid,...){

  if(is.null(grid)){
    model.fit <- train(diabetes~.,
                     data = training,
                     method = method,
                     trControl = control,
                     ...)
    return(model.fit)
  }

  else{
    model.fit <- train(diabetes~.,
                     data = training,
                     method = method,
                     trControl = control,
                     tuneGrid = grid,
                     ...)
    return(model.fit)
  }
}

control <- trainControl(method = "cv", number = 10, 
                        classProbs = TRUE,
                        summaryFunction = prSummary)
```


```{r}

#Decision Tree
dtree <- model("rpart",train, control, grid=NULL, metric="AUC", tuneLength=10)
dtree

dtree.pred <- predict(dtree, test, type="prob")
dtree.pred %>% head
#probability for class positive 1 = pos
dtree.pred1 <- dtree.pred[,2] 
#if cutoff value is 0.5, and pred1 < 0.5, then it's 0 = neg for diabetes
dtree.pred1 %>% head



#roc built in R
pred.dtree <- prediction(dtree.pred1, test$diabetes, label.ordering = c("neg","pos"))

#ROC curve
perf <- performance(pred.dtree, "tpr","fpr")
plot(perf)

#AUC value
perf <- performance(pred.dtree, "auc")
perf@y.values[[1]]

#Sensitivity vs Specificity
perf <- performance(pred.dtree, "sens","spec")
plot(perf)

#Precision vs Recall
perf <- performance(pred.dtree, "prec", "rec")
plot(perf)

#Another way
roc.curve.dtree <- roc(test$diabetes, dtree.pred1, levels=c("pos","neg"), positive="pos")
plot(roc.curve.dtree)
auc(roc.curve.dtree)
roc.curve.dtree %>% coords("best", transpose=FALSE)

#Optimal Cutoff probability
threshold.dtree <- data.frame(roc.curve.dtree %>% coords("best", transpose=FALSE))[,1]
threshold.dtree

#We would want higher probability of predicting "pos",
#since it's to predict if the patient has diabetes

#Typical cutoff value, 0.5
confusionMatrix(as.factor(ifelse(dtree.pred1 >= 0.5, "pos", "neg")), 
                as.factor(test$diabetes))

#Optimal cutoff value
confusionMatrix(as.factor(ifelse(dtree.pred1 >= threshold.dtree, "pos", "neg")),
                as.factor(test$diabetes))

#Functions created by my own
tp.dat.dtree <- tptnfpfn(test$diabetes,ifelse(dtree.pred1 >= threshold.dtree, "pos", "neg"))
tp.dat.dtree

precision(tp.dat.dtree)
recall(tp.dat.dtree)
spec(tp.dat.dtree)
f1.score(tp.dat.dtree)

roc.dat.dtree <- roc.func(test$diabetes, dtree.pred1)
roc.dat.dtree %>% head
roc.dat.dtree %>% ggplot(aes(x=FPR, y=TPR)) + geom_line() + geom_abline() +
  labs(title="ROC Curve")
roc.dat.dtree %>% ggplot(aes(x=TPR, y=Spec)) + geom_line() +
  labs(title = "Sensitivity vs Specificity") + xlab("Sensitivity") + ylab("Specificity")
roc.dat.dtree %>% ggplot(aes(x=TPR, y=Precision)) + geom_line() +
  labs(title = "Recall vs Precision") + xlab("Recall") + ylab("Precision")


auc.dtree <- 1-auc.func(test$diabetes, dtree.pred1)
auc.dtree
```

```{r}

#Random Forest
rf <- model("rf",train, control, grid=NULL, metric="AUC", tuneLength=10)
rf

rf.pred <- predict(rf, test, type="prob")
rf.pred %>% head
#probability for 1 = neg
rf.pred1 <- rf.pred[,2]
#if cutoff value is 0.5, and pred1 < 0.5, then it's 0 = pos for diabetes
rf.pred1 %>% head



#roc built in R
pred.rf <- prediction(rf.pred1, test$diabetes, label.ordering = c("neg","pos"))

#ROC curve
perf <- performance(pred.rf, "tpr","fpr")
plot(perf)

#AUC value
perf <- performance(pred.rf, "auc")
perf@y.values[[1]]

#Sensitivity vs Specificity
perf <- performance(pred.rf, "sens","spec")
plot(perf)

#Precision vs Recall
perf <- performance(pred.rf, "prec", "rec")
plot(perf)

#Another way
roc.curve.rf <- roc(test$diabetes, rf.pred1, levels=c("pos","neg"), positive="pos")
plot(roc.curve.rf)
auc(roc.curve.rf)
roc.curve.rf %>% coords("best", transpose=FALSE)

#Optimal Cutoff probability
threshold.rf <- data.frame(roc.curve.rf %>% coords("best", transpose=FALSE))[,1]
threshold.rf

#We would want higher probability of predicting "pos",
#since it's to predict if the patient has diabetes

#Typical cutoff value, 0.5
confusionMatrix(as.factor(ifelse(rf.pred1 >= 0.5, "pos", "neg")), 
                as.factor(test$diabetes))

#Optimal cutoff value
confusionMatrix(as.factor(ifelse(rf.pred1 >= threshold.rf, "pos", "neg")),
                as.factor(test$diabetes))

#Functions created by my own
tp.dat.rf <- tptnfpfn(test$diabetes,ifelse(rf.pred1 >= threshold.rf, "pos", "neg"))
tp.dat.rf

precision(tp.dat.rf)
recall(tp.dat.rf)
spec(tp.dat.rf)
f1.score(tp.dat.rf)

roc.dat.rf <- roc.func(test$diabetes, rf.pred1)
roc.dat.rf %>% head
roc.dat.rf %>% ggplot(aes(x=FPR, y=TPR)) + geom_line() + geom_abline() +
  labs(title="ROC Curve")
roc.dat.rf %>% ggplot(aes(x=TPR, y=Spec)) + geom_line() +
  labs(title = "Sensitivity vs Specificity") + xlab("Sensitivity") + ylab("Specificity")
roc.dat.rf %>% ggplot(aes(x=TPR, y=Precision)) + geom_line() +
  labs(title = "Recall vs Precision") + xlab("Recall") + ylab("Precision")


auc.rf <- 1-auc.func(test$diabetes, rf.pred1)
auc.rf
```




```{r}

#Logistic Regression
glm.mod <- model("glm", train, control, grid=NULL, metric= "AUC")
glm.mod

glm.pred <- predict(glm.mod, test, type="prob")
glm.pred %>% head

glm.pred1 <- glm.pred[,2]
glm.pred1 %>% head

#roc built in R
pred.glm <- prediction(glm.pred1, test$diabetes, label.ordering = c("neg","pos"))

#ROC curve
perf <- performance(pred.glm, "tpr","fpr")
plot(perf)

#AUC value
perf <- performance(pred.glm, "auc")
perf@y.values[[1]]

#Sensitivity vs Specificity
perf <- performance(pred.glm, "sens","spec")
plot(perf)

#Precision vs Recall
perf <- performance(pred.glm, "prec", "rec")
plot(perf)

#Another way
roc.curve.glm <- roc(test$diabetes, glm.pred1, levels=c("pos","neg"), positive="pos")
plot(roc.curve.glm)
auc(roc.curve.glm)
roc.curve.glm %>% coords("best", transpose=FALSE)

#Optimal Cutoff probability
threshold.glm <- data.frame(roc.curve.glm %>% coords("best", transpose=FALSE))[,1]
threshold.glm

#We would want higher probability of predicting "pos",
#since it's to predict if the patient has diabetes

#Typical cutoff value, 0.5
confusionMatrix(as.factor(ifelse(glm.pred1 >= 0.5, "pos", "neg")), 
                as.factor(test$diabetes))

#Optimal cutoff value
confusionMatrix(as.factor(ifelse(glm.pred1 >= threshold.glm, "pos", "neg")),
                as.factor(test$diabetes))

#Functions created by my own
tp.dat.glm <- tptnfpfn(test$diabetes,ifelse(glm.pred1 >= threshold.glm, "pos", "neg"))
tp.dat.glm

precision(tp.dat.glm)
recall(tp.dat.glm)
spec(tp.dat.glm)
f1.score(tp.dat.glm)

roc.dat.glm <- roc.func(test$diabetes, glm.pred1)
roc.dat.glm %>% head
roc.dat.glm %>% ggplot(aes(x=FPR, y=TPR)) + geom_line() + geom_abline() +
  labs(title="ROC Curve")
roc.dat.glm %>% ggplot(aes(x=TPR, y=Spec)) + geom_line() +
  labs(title = "Sensitivity vs Specificity") + xlab("Sensitivity") + ylab("Specificity")
roc.dat.glm %>% ggplot(aes(x=TPR, y=Precision)) + geom_line() +
  labs(title = "Recall vs Precision") + xlab("Recall") + ylab("Precision")


auc.glm <- 1-auc.func(test$diabetes, glm.pred1)
auc.glm
```




```{r}

#Support Vector Machine with RBF Kernel
svm.mod <- model("svmRadial", train, control, grid=NULL, metric="AUC", tuneLength=10)
svm.mod

svm.pred <- predict(svm.mod, test, type="prob")
svm.pred %>% head

svm.pred1 <- svm.pred[,2]
svm.pred1 %>% head


#roc built in R
pred.svm <- prediction(svm.pred1, test$diabetes, label.ordering = c("neg","pos"))

#ROC curve
perf <- performance(pred.svm, "tpr","fpr")
plot(perf)

#AUC value
perf <- performance(pred.svm, "auc")
perf@y.values[[1]]

#Sensitivity vs Specificity
perf <- performance(pred.svm, "sens","spec")
plot(perf)

#Precision vs Recall
perf <- performance(pred.svm, "prec", "rec")
plot(perf)

#Another way
roc.curve.svm <- roc(test$diabetes, svm.pred1, levels=c("pos","neg"), positive="pos")
plot(roc.curve.svm)
auc(roc.curve.svm)
roc.curve.svm %>% coords("best", transpose=FALSE)

#Optimal Cutoff probability
threshold.svm <- data.frame(roc.curve.svm %>% coords("best", transpose=FALSE))[,1]
threshold.svm

#We would want higher probability of predicting "pos",
#since it's to predict if the patient has diabetes

#Typical cutoff value, 0.5
confusionMatrix(as.factor(ifelse(svm.pred1 >= 0.5, "pos", "neg")), 
                as.factor(test$diabetes))

#Optimal cutoff value
confusionMatrix(as.factor(ifelse(svm.pred1 >= threshold.svm, "pos", "neg")),
                as.factor(test$diabetes))

#Functions created by my own
tp.dat.svm <- tptnfpfn(test$diabetes,ifelse(svm.pred1 >= threshold.svm, "pos", "neg"))
tp.dat.svm

precision(tp.dat.svm)
recall(tp.dat.svm)
spec(tp.dat.svm)
f1.score(tp.dat.svm)

roc.dat.svm <- roc.func(test$diabetes, svm.pred1)
roc.dat.svm %>% head
roc.dat.svm %>% ggplot(aes(x=FPR, y=TPR)) + geom_line() + geom_abline() +
  labs(title="ROC Curve")
roc.dat.svm %>% ggplot(aes(x=TPR, y=Spec)) + geom_line() +
  labs(title = "Sensitivity vs Specificity") + xlab("Sensitivity") + ylab("Specificity")
roc.dat.svm %>% ggplot(aes(x=TPR, y=Precision)) + geom_line() +
  labs(title = "Recall vs Precision") + xlab("Recall") + ylab("Precision")


auc.svm <- 1-auc.func(test$diabetes, svm.pred1)
auc.svm
```


```{r}
roc.dat.dtree$model <- "Decision Tree"
roc.dat.rf$model <- "Random Forest"
roc.dat.glm$model <- "Logistic Regression"
roc.dat.svm$model <- "SVM with RBF"

roc.dat <- rbind(roc.dat.dtree,
                 roc.dat.rf,
                 roc.dat.glm,
                 roc.dat.svm)

#ROC Curves
roc.dat %>% ggplot(aes(x=FPR, y=TPR, col=model)) + 
  geom_line() + 
  geom_abline() +
  labs(title= "ROC Curves for 4 models")


#Optimal Cutoff Values by models and by metrics

#Decision Tree

#Optimal Cutoff for Sensitivity and Specificity
roc.dat.dtree$cutoff[which.max(roc.dat.dtree$TPR + roc.dat.dtree$Spec)]

#Optimal Cutoff for Precision and Recall
roc.dat.dtree$cutoff[which.max(roc.dat.dtree$TPR + roc.dat.dtree$Precision)]

roc.dat.dtree %>% filter(cutoff == roc.dat.dtree$cutoff[which.max(roc.dat.dtree$TPR + roc.dat.dtree$Spec)]) %>% head
roc.dat.dtree %>% filter(cutoff == roc.dat.dtree$cutoff[which.max(roc.dat.dtree$TPR + roc.dat.dtree$Precision)]) %>% head

roc.curve.dtree %>% 
  coords("best",transpose=FALSE)

#Random Forest

roc.dat.rf$cutoff[which.max(roc.dat.rf$TPR + roc.dat.rf$Spec)]
roc.dat.rf$cutoff[which.max(roc.dat.rf$TPR + roc.dat.rf$Precision)]

roc.dat.rf %>% filter(cutoff == roc.dat.rf$cutoff[which.max(roc.dat.rf$TPR + roc.dat.rf$Spec)])
roc.dat.rf %>% filter(cutoff == roc.dat.rf$cutoff[which.max(roc.dat.rf$TPR + roc.dat.rf$Precision)])

roc.curve.rf %>% 
  coords("best",transpose=FALSE)

#Logistic Regression
roc.dat.glm$cutoff[which.max(roc.dat.glm$TPR + roc.dat.glm$Spec)]
roc.dat.glm$cutoff[which.max(roc.dat.glm$TPR + roc.dat.glm$Precision)]

roc.dat.glm %>% filter(cutoff == roc.dat.glm$cutoff[which.max(roc.dat.glm$TPR + roc.dat.glm$Spec)])
roc.dat.glm %>% filter(cutoff == roc.dat.glm$cutoff[which.max(roc.dat.glm$TPR + roc.dat.glm$Precision)])

roc.curve.glm %>% 
  coords("best",transpose=FALSE)

#SVM
roc.dat.svm$cutoff[which.max(roc.dat.svm$TPR + roc.dat.svm$Spec)]
roc.dat.svm$cutoff[which.max(roc.dat.svm$TPR + roc.dat.svm$Precision)]

roc.dat.svm %>% filter(cutoff == roc.dat.svm$cutoff[which.max(roc.dat.svm$TPR + roc.dat.svm$Spec)])
roc.dat.svm %>% filter(cutoff == roc.dat.svm$cutoff[which.max(roc.dat.svm$TPR + roc.dat.svm$Precision)])

roc.curve.svm %>% 
  coords("best",transpose=FALSE)


#AUC
auc.dtree
auc.rf
auc.glm
auc.svm

table(train$diabetes)
table(test$diabetes)
#Since the dataset is an imablanced data, so I will choose the model by the best cutoff for Precision and Recall, even if the model has less accuracy and less AUC values than others. 

#So I will see the f1 score
dtree.cutoff <- roc.dat.dtree$cutoff[which.max(roc.dat.dtree$f1score)]
rf.cutoff <- roc.dat.rf$cutoff[which.max(roc.dat.rf$f1score)]
glm.cutoff <- roc.dat.glm$cutoff[which.max(roc.dat.glm$f1score)]
svm.cutoff <- roc.dat.svm$cutoff[which.max(roc.dat.svm$f1score)]

#Confusion Matrix with Optimal cutoff value
confusionMatrix(as.factor(ifelse(dtree.pred1 >= dtree.cutoff, "pos", "neg")),
                as.factor(test$diabetes))

#Optimal cutoff value
confusionMatrix(as.factor(ifelse(rf.pred1 >= rf.cutoff, "pos", "neg")),
                as.factor(test$diabetes))

#Optimal cutoff value
confusionMatrix(as.factor(ifelse(glm.pred1 >= glm.cutoff, "pos", "neg")),
                as.factor(test$diabetes))

#Optimal cutoff value
confusionMatrix(as.factor(ifelse(svm.pred1 >= svm.cutoff, "pos", "neg")),
                as.factor(test$diabetes))


pred.f1score <- data.frame(dtree = max(roc.dat.dtree$f1score),
                           rf = max(roc.dat.rf$f1score),
                           glm = max(roc.dat.glm$f1score),
                           svm = max(roc.dat.svm$f1score))

pred.f1score
which.max(pred.f1score) %>% names
#glm seems to be the best model for this imbalanaced dataset

#with auc
auc.dat <- data.frame(dtree = auc.dtree,
                      rf = auc.rf,
                      glm = auc.glm,
                      svm = auc.svm)
auc.dat
which.max(auc.dat) %>% names


```